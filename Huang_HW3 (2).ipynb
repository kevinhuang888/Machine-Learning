{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCGUKkwUOLxM"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjBhqvLYOLxQ"
      },
      "source": [
        "# Homework 3: Neural Networks\n",
        "\n",
        "Please read all instructions for this homework carefully. Many of the questions ask for specific things to be in your notebooks and many of your questions may be answered in the step-by-step instructions. You will need include _all_ parts of the questions in your answers to recieve full credit.\n",
        "\n",
        "This homework is going to focus heavily on the practical use of neural networks. You will be required to learn the basics of PyTorch (https://pytorch.org/) to implement and train fully connected and convolutional neural networks.\n",
        "\n",
        "We strongly recommend working through the PyTorch beginner guide [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), which will both give you a good idea of how PyTorch works and provide you with some boilerplate code you can adapt for this assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1HsSSJ79d8c"
      },
      "source": [
        "## Question 1: Backpropagation\n",
        "\n",
        "A computational graph is a framework to represent complex mathematical expressions as a directed graph of function compositions, starting from the simplest, to the most complex. For instance, the expression $e = (a+b)\\times(b+1)$ can be represented as a computational graph, as shown in the figure below. Each node represents a variable, where the direction represents the flow of inputs to (intermediate) outputs. All incoming variables are collected to be operated on by a function. \n",
        "\n",
        "$$\n",
        "c = a + b \\\\\n",
        "d = b + 1 \\\\\n",
        "e = c \\times d\n",
        "$$\n",
        "\n",
        "\n",
        "In the graph below, these functions are multiplications and additions, and a sequence of simple operations leads to the relatively complex expression of $e$.\n",
        "\n",
        "![A Computational Graph (http://colah.github.io/posts/2015-08-Backprop/)](http://colah.github.io/posts/2015-08-Backprop/img/tree-def.png)\n",
        "\n",
        "Computational graphs allow for a computational abstraction to compute _exact_ partial derivatives of complex expressions, as a realization of the chain rule from calculus. For instance, computing $\\frac{\\partial e}{\\partial a}$ involves essentially reversing the arrows and transporting intermediate partial derivatives back to $a$. This method is termed _backpropagation_ (short: _backprop_), and is a key ingredient of modern neural network frameworks that implement gradient descent (including PyTorch). The first-three sections of [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/) should allow reasonable understanding for the next questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_M9dWWhK5nY"
      },
      "source": [
        "### Part A: Computational Graph for Logistic Regression\n",
        "\n",
        "Build the computational graph of logistic regression for a single input/output pair. As in earlier homeworks/labs, use the variables $\\mathbf{w} = \\begin{bmatrix}\\mathbf{w}_0 & \\mathbf{w}_1 & \\cdots & \\mathbf{w}_d\\end{bmatrix}^T \\in \\mathbb{R}^{d+1}$ for the vector of parameters and $\\mathbf{x} = \\begin{bmatrix}1 & x_1 & x_2 & \\cdots & x_d \\end{bmatrix} \\in \\mathbb{R}^{d}$ for the vector input, with $y$ as the corresponding output. Write the computational graph as a sequence of algebraic equations in terms of simple unary/binary functions only (no graphic needed).\n",
        "\n",
        "**Answer Here:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7CX99RMTerQ"
      },
      "source": [
        "$w, x \\implies a = wx \\implies y = \\sigma(a) = \\frac{1}{1+e^{-(a)}}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gV349FCK8Vf"
      },
      "source": [
        "### Part B: Backprop on Logistic Regression\n",
        "\n",
        "For the computational graph built in I., compute the derivatives of the loss in logistic regression with respect to the parameters $\\mathbf{w}$ via backpropagation. Write these as a sequence of equations (no graphic needed), that clearly show _backward_ flow of gradients via the chain rule.\n",
        "\n",
        "**Hint**: Table 3 of [this manuscript](https://arxiv.org/abs/1502.05767) may be a helpful resource.\n",
        "\n",
        "**Answer Here:** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1hjFAkOcV9a"
      },
      "source": [
        "To avoid overloading notation, let z represent y from Part A such that\n",
        "\n",
        "$$z = \\sigma(a) = \\frac{1}{1+e^{-a}}$$\n",
        "\n",
        "$\\frac{dz}{da} = \\frac{e^{-a}}{(1+e^{-a})^2}$ (quotient rule) $= \\frac{(e^{-a} + 1) - 1}{(1+e^{-a})^2} = \\frac{(1 + e^{-a})}{(1+e^{-a})^2} - \\frac{1}{(1+e^{-a})^2}$\n",
        "\n",
        "$= \\frac{1}{1+e^{-a}} - \\frac{1}{(1+e^{-a})^2} = \\frac{1}{1+e^{-a}}(1-\\frac{1}{1+e^{-a}}) = \\sigma(a)(1-\\sigma(a)) = z(1-z)$\n",
        "\n",
        "$P(D|w) = \\displaystyle \\prod_{i=1}^{n} \\sigma(wx)^{y_i}[1-\\sigma(wx)]^{(1-y_i)}$ \n",
        "\n",
        "$\\implies L = log(P(D|w)) = \\displaystyle \\sum_{i=1}^{n} y_ilog(\\sigma(a)) + (1-y_i)log(1-\\sigma(a)) = \\displaystyle \\sum_{i=1}^{n} (y_ilog(z) + (1-y_i)log(1-z)) = \\displaystyle \\sum_{i=1}^{n} y_ilog(z) + \\displaystyle \\sum_{i=1}^{n}(1-y_i)log(1-z)$\n",
        "\n",
        "$\\frac{dL}{dw} = \\displaystyle \\sum_{i=1}^{n} y_i\\frac{1}{z} \\frac{dz}{da} \\frac{da}{dw} - \\displaystyle \\sum_{i=1}^{n} (1-y_i)\\frac{1}{1-z} \\frac{dz}{da} \\frac{da}{dw} = \\displaystyle \\sum_{i=1}^{n} y_i\\frac{1}{z} z(1-z) x_i - \\displaystyle \\sum_{i=1}^{n} (1-y_i)\\frac{1}{1-z} z(1-z) x_i = \\displaystyle \\sum_{i=1}^{n} y_i(1-z) x_i - \\displaystyle \\sum_{i=1}^{n} (1-y_i) z x_i= \\displaystyle \\sum_{i=1}^{n} (y_i - y_iz)x_i - zx_i + y_izx_i$\n",
        "\n",
        "$= \\displaystyle \\sum_{i=1}^{n} x_i(y_i - y_iz - z + y_iz) = \\displaystyle \\sum_{i=1}^{n} x_i(y_i - z) = \\displaystyle \\sum_{i=1}^{n} x_i(y_i - \\sigma(wx))$\n",
        "\n",
        "The gradients we used to demonstrate backward flow:\n",
        "\n",
        " $$\\frac{dL}{dw} = \\frac{dL}{dz} \\frac{dz}{da} \\frac{da}{dw}$$\n",
        "\n",
        " Note: $\\frac{dL}{dz}$ was just the $\\frac{dL}{dw}$ equation without $\\frac{dz}{da} \\frac{da}{dw}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj2Eh2bxK-T-"
      },
      "source": [
        "### Part C: Challenges in _deep_ neural networks\n",
        "\n",
        "Now consider a _deep_ neural network with L layers, such that the function is defined via weight matrices at each layer are defined as $W^{(l)}$ for all $l \\in [1, \\dots, L]$, along with $\\sigma$ (sigmoid) non-linearity which is applied pointwise to the activation $h^{(l)}$ of each layer.\n",
        "\n",
        "$$\n",
        "h^{(l)} = \\begin{cases}\n",
        "\\sigma(W^{(l)}\\mathbf{x}),\\quad\\quad\\text{if}~l = 1 \\\\\n",
        "\\sigma(W^{(l)}h^{(l-1)}),\\quad\\text{if}~l < L \\\\\n",
        "W^{(l)}h^{(l-1)},\\quad\\quad\\text{if}~l = L\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The definition simply defines the operations at the input layer (which involves $\\mathbf{x}$), hidden layers (which only involve activations at the previous layer $h^{(l-1)}$), and the output layer where no $\\sigma$ (sigmoid) activation is applied. Therefore, the neural network can be recursively defined as $f_\\mathbf{w}(h^{(L)})$. Why can very deep networks (i.e. large $L$) with sigmoid non-linearities be hard to learn via backpropagation?\n",
        "\n",
        "**Hint**: Consider a two hidden layer neural network (i.e. L = 3), and think about how repeated application of chain rule affects the qualitative nature of the gradients as we backpropagate from the output layer to the input layer, as we increase L.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPkhfZWPTerR"
      },
      "source": [
        "**Answer Here:** \n",
        "\n",
        "Taking the hint into account (L = 3), $$h(L) = W^{(l)}\\sigma(W^{(l)}\\sigma(W^{(l)}\\mathbf{x}))$$ \n",
        "\n",
        "This is basically\n",
        "$$\\frac{1}{1+e^{-(\\frac{1}{1+e^{-(Wx)}})}}$$\n",
        "\n",
        "if we just look at the activation functions.\n",
        "\n",
        "\n",
        "In other words, our loss becomes increasingly smaller as our neural network gets deeper. This means that our gradients become smaller as well during backpropagation causing gradient-descent to have a higher chance of prematurely concluding a non-optimal solution. We have seen that SGD is more adept at avoiding these local minima, but for large amounts of paramters, SGD can get thrown off as well. There are other backpropagation methods out there, but the point is that with large neural networks, all of these drawbacks are brought out simply due to the increase in complexity and depth of our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1CXexSOLxR"
      },
      "source": [
        "## Question 2: A Simple Dataset\n",
        "\n",
        "The aim of this question is to implement a first neural network, and start to get an intuition about what sorts of functions neural networks are capable of producing.\n",
        "First, use the provided code to generate 1000 training points and 500 testing points from the _two spirals_ dataset with `noise=1.5`.\n",
        "\n",
        "\n",
        "**Note** We've also included a two functions to help you. You should _not_ alter these functions (nor should you need to).\n",
        "\n",
        "`plotter` takes in your model and the training and testing data and will plot the predictions generated by your neural network.\n",
        "\n",
        "`accuracy` is a simple function that will compute the accuracy of your model on some given `x` and `y` data, i.e. either `train_x` and `train_y`, or `test_x` and `test_y`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCKHtk7yOLxR"
      },
      "source": [
        "def twospirals(n_points, noise=1.5, random_state=42):\n",
        "    \"\"\"\n",
        "     Returns the two spirals dataset.\n",
        "     Note: n_points is points PER CLASS\n",
        "    \"\"\"\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 600 * (2*np.pi)/360\n",
        "    d1x = -1.5*np.cos(n)*n + np.random.randn(n_points,1) * noise\n",
        "    d1y =  1.5*np.sin(n)*n + np.random.randn(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))),\n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
        "\n",
        "ntrain = 1000\n",
        "ntest = 500\n",
        "noise = 1.5\n",
        "\n",
        "train_x, train_y = twospirals(int(ntrain/2), noise=noise)\n",
        "train_x, train_y = torch.FloatTensor(train_x), torch.FloatTensor(train_y).unsqueeze(-1)\n",
        "\n",
        "test_x, test_y = twospirals(int(ntest/2), noise=noise)\n",
        "test_x, test_y = torch.FloatTensor(test_x), torch.FloatTensor(test_y).unsqueeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K45OnsTOLxR"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DBRwX-8OLxR"
      },
      "source": [
        "def plotter(model, train_x, train_y, test_x, test_y):\n",
        "    '''\n",
        "    This is just a simple plotting function, you should NOT need to change anything here\n",
        "    '''\n",
        "    buffer = 1.\n",
        "    h = 0.1\n",
        "    x_min, x_max = train_x[:, 0].min() - buffer, train_x[:, 0].max() + buffer\n",
        "    y_min, y_max = train_x[:, 1].min() - buffer, train_x[:, 1].max() + buffer\n",
        "\n",
        "    xx,yy=np.meshgrid(np.arange(x_min.cpu(), x_max.cpu(), h), \n",
        "                      np.arange(y_min.cpu(), y_max.cpu(), h))\n",
        "    in_grid = torch.FloatTensor([xx.ravel(), yy.ravel()]).t()\n",
        "\n",
        "    pred = torch.sigmoid(model(in_grid)).detach().reshape(xx.shape)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    cmap = sns.color_palette(\"crest_r\", as_cmap=True)\n",
        "    plt.contourf(xx, yy, pred, alpha=0.5, cmap=cmap)\n",
        "    plt.title(\"Classifier\", fontsize=24)\n",
        "    cbar= plt.colorbar()\n",
        "    cbar.set_label(label=r\"$P(Y = 1)$\", size=18)\n",
        "    cbar.ax.tick_params(labelsize=18)\n",
        "    plt.scatter(train_x[:, 0].cpu(), train_x[:, 1].cpu(), c=train_y[:, 0].cpu(), cmap=plt.cm.binary, alpha=0.5, label=\"Train\")\n",
        "    plt.scatter(test_x[:, 0].cpu(), test_x[:, 1].cpu(), c=test_y[:, 0].cpu(), cmap=plt.cm.binary, marker='+', s=150, label=\"Test\")\n",
        "    plt.legend(fontsize=18)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "    \n",
        "def accuracyy(model, X, Y):\n",
        "    \n",
        "    preds = torch.round(torch.sigmoid((model(X))))\n",
        "    return 100 * len(torch.where(preds == Y)[0])/Y.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opdwwK2DOLxS"
      },
      "source": [
        "### Part A: Network Definition\n",
        "\n",
        "Write a class that defines a _fully connected_ neural network with ReLU activations. Your class should be a child class of the `torch.nn.Module` class, i.e. the first line of your class definition should be something like:\n",
        "```{python}\n",
        "class NeuralNetwork(nn.Module):\n",
        "```\n",
        "The number of layers and width of each layer is up to you - but keep this implementation flexible, as you'll need to come back and tweak your network in order to achieve the desired accuracy on our dataset.\n",
        "Additionally, this network should be built to work with the two spirals dataset, meaning it should take 2 dimensional inputs and produce a 1 dimensional output. Specifically we are constructing a network $f(x): \\mathbb{R}^2 \\rightarrow \\mathbb{R}$.\n",
        "\n",
        "To ensure that your model is correctly defined, the last cell in your notebook for this section should contain the following lines, with the correct handle for your neural network class:\n",
        "\n",
        "```{python}\n",
        "model = your_network_class(appropriate_arguments)\n",
        "print(model(train_x).shape)\n",
        "```\n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7WARaefTerT"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,in_size,bias = True):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 100)\n",
        "        self.fc4 = nn.Linear(100, 100)\n",
        "        #self.fc5 = nn.Linear(10, 10)\n",
        "       # self.fc6 = nn.Linear(10, 10)\n",
        "        #self.fc7 = nn.Linear(10, 10)\n",
        "        #self.fc8 = nn.Linear(10, 10)\n",
        "        self.fc9 = nn.Linear(100,1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "       # x = F.relu(self.fc5(x))\n",
        "       # x = F.relu(self.fc6(x))\n",
        "       # x = F.relu(self.fc7(x))\n",
        "       # x = F.relu(self.fc8(x))\n",
        "        x = self.fc9(x)\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh7jtYanTerU"
      },
      "source": [
        "model = NeuralNetwork(in_size = train_x.size(-1))\n",
        "print(model(train_x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCmHCDDeOLxT"
      },
      "source": [
        "### Part B: Training\n",
        "\n",
        "Write and execute code to train your neural network. You should train with SGD, using the `torch.nn.BCEWithLogitsLoss` loss function. Tune your network and training routine until you get **at least 90% training accuracy**. You should think about changing: width and depth of your network, the learning rate of your optimizer, the number of training iterations, and including bias terms in your network layers.\n",
        "\n",
        "After your model is trained plot the predicted function using the `plotter` function. Report the _test_ accuracy achieved by your model as well as the architecture of your network - how many layers did you use, how wide was each layer, did you include a bias term, etc.\n",
        "\n",
        "\n",
        "**Note**: with the right network setup you can achieve this training accuracy in less than 5 seconds on a laptop, you can use a large model that takes longer to train if you want, but be advised that you don't _need_ a large model to be successful here. \n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20UyHtdYTerV"
      },
      "source": [
        "optim = torch.optim.SGD(model.parameters(), lr=12e-2)\n",
        "ell = nn.BCEWithLogitsLoss()\n",
        "epochs = 2000\n",
        "\n",
        "loss_trace = []\n",
        "for e in range(epochs):\n",
        "    n_correct = 0\n",
        "    optim.zero_grad()\n",
        "    p_y = model(train_x)\n",
        "    loss = ell(p_y, train_y.float())\n",
        "    loss_trace.append(loss.detach().item()) \n",
        "    loss.backward()\n",
        "    optim.step()\n",
        " \n",
        "    acc = accuracyy(model, train_x, train_y)\n",
        "    if (e + 1) % 200 == 0:\n",
        "        print(f'[Epoch {e + 1}] Loss: {loss_trace[-1]} Accuracy = {acc}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7E3Mp9ZTerV"
      },
      "source": [
        "plotter(model, train_x, train_y, test_x, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0v2fCDQTerW"
      },
      "source": [
        "test_acc = accuracyy(model, test_x, test_y)\n",
        "print(test_acc, \"Layers = 5,\", \"Width of Layer = 100,\", \"Bias = true,\", \"2000 Epochs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HIVNc1LOLxV"
      },
      "source": [
        "## Question 3: Convolutional Networks and Image Recognition\n",
        "\n",
        "In this question we are going to build up to larger models on larger datasets. If you don't already have access to a CUDA-enabled GPU you will need to complete this section of the homework on Google Colab. Google Colab allows you to run hardware accelerated notebooks online for free. By uploading this notebook into Colab then selecting `Runtime` -> `Change Runtime Type` -> And choosing `GPU` as your hardware accelerator you will be able to run your code on a GPU.\n",
        "\n",
        "Before starting we need to setup our data. We'll use the `torchvision` package to handle sourcing and normalizing the CIFAR-10 dataset, which we'll need to download to continue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPe3yTOU2bw"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8AatrNEVCTD"
      },
      "source": [
        "The code cell below sets up a data transformation which casts the data to tensors (so we can pass it through a network), and normalizes each of the 3 RGB layers of the images. The first time you run this you will see a progress bar showing that the dataset is being downloaded. \n",
        "\n",
        "After running this code you will have a `trainloader` and a `testloader`. These are iterable python objects that allow you to sample mini-batches of data according the `batch_size` you passed in to the `DataLoader` function call. You can loop through the full dataset by running:\n",
        "```{python}\n",
        "for inputs, targets in trainloader:\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6DBwhTqOLxV"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                         pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g4HUHGNZomQ"
      },
      "source": [
        "## Part A: Network Definition\n",
        "\n",
        "A common strategy for defining convolutional neural networks is to first pass the data through a set of convolutional layers to perform feature extraction, then to pass through a set of linear, or fully connected, layers to perform classification. \n",
        "\n",
        "Construct a class that defines a pytorch model to be used with CIFAR-10 containing the following layers, in order:\n",
        "\n",
        "- A convolutional layer outputting 8 channels with a kernel size of 3\n",
        "- A ReLU activation\n",
        "- A max-pooling layer with kernel size of 2\n",
        "- A convolutional layer outputting 16 channels with a kernel size of 3\n",
        "- A ReLU activation\n",
        "- A max-pooling layer with kernel size of 2\n",
        "- A convolutional layer outputting 32 channels with a kernel size of 3\n",
        "- A ReLU activation\n",
        "- A max-pooling layer with kernel size of 2\n",
        "  - After this operation you will need to reshape the outputs of the max-pooling layer to a tensor of size `batch_size` $\\times$ `128`\n",
        "- A linear layer with an output size of 128\n",
        "- A ReLU activation\n",
        "- A linear layer with an output size of 64\n",
        "- A ReLU activation\n",
        "- A linear layer with an output size of 10\n",
        "\n",
        "\n",
        "\n",
        "To show that your network is correctly implemented, after you define this class, create an instance of it and show that if you pass a batch of CIFAR-10 images from `trainloader` through the network you get an output of size `batch_size` $\\times$ `10`.\n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw1aXhCXTerY"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,bias = False):\n",
        "        super().__init__()\n",
        "        self.conv8 = nn.Conv2d(3,8,3)\n",
        "        self.conv16 = nn.Conv2d(8,16,3)\n",
        "        self.conv32 = nn.Conv2d(16,32,3)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        self.fc128 = nn.Linear(128, 128)\n",
        "        self.fc64 = nn.Linear(128, 64)\n",
        "        self.fc10 = nn.Linear(64,10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.pool(F.relu(self.conv8(x)))\n",
        "        x = self.pool(F.relu(self.conv16(x)))\n",
        "        x = self.pool(F.relu(self.conv32(x)))\n",
        "        x = x.view(-1, 128)\n",
        "        \n",
        "        \n",
        "        x = F.relu(self.fc128(x))\n",
        "        x = F.relu(self.fc64(x))\n",
        "        x = self.fc10(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-k5TAehTerZ"
      },
      "source": [
        "for inputs, targets in trainloader:\n",
        "    break;\n",
        "print(inputs.shape)\n",
        "print(targets.shape)\n",
        "net = ConvNet()\n",
        "print(net(inputs).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP23T_irybRd"
      },
      "source": [
        "## Part B: Training Loop\n",
        "\n",
        "Using `torch.nn.CrossEntropyLoss` as your objective function and `torch.optim.SGD` as your optimizer. Train your network for 2 epochs while running on a _CPU_ using a batch size of 128. How long does it take? \n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A3RKVB4TerZ"
      },
      "source": [
        "%%time\n",
        "coptim = torch.optim.SGD(net.parameters(), lr=4e-2)\n",
        "cel = nn.CrossEntropyLoss()\n",
        "cepochs = 2\n",
        "\n",
        "for e in range(cepochs):\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        coptim.zero_grad()\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        loss = cel(outputs,targets)\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(running_loss/2000)\n",
        "        running_loss = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV4B00HeTerZ"
      },
      "source": [
        "*It is very slow.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oajg3e4Ry5zx"
      },
      "source": [
        "## Part C: Porting to a GPU\n",
        "\n",
        "CUDA-enabled GPUs can speed up training time signficantly, and have really enabled the widespread success of neural networks. Using a GPU, reinitialize your network and train for 2 epochs using a batch size of 128. How long does 2 epochs of training take? If you are correctly utilizing a GPU a single epoch should take only about 50-60% as much time as in Part B. \n",
        "\n",
        "To be sure you are doing things correctly: if you have a torch tensor, `X`, you can see what device it is on using the command `X.device`; if the tensor is stored on the GPU you will see `type='cuda'` in the output.\n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Qq96xLTerb"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r13mwxkWUVc0"
      },
      "source": [
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFOPs6WNU4UO"
      },
      "source": [
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rhserQfVJIx"
      },
      "source": [
        "inputs.device\n",
        "targets.device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_wGeBu4VQC-"
      },
      "source": [
        "%%time\n",
        "coptim = torch.optim.SGD(net.parameters(), lr=4e-2)\n",
        "cel = nn.CrossEntropyLoss()\n",
        "cepochs = 2\n",
        "\n",
        "for e in range(cepochs):\n",
        "    running_loss = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        coptim.zero_grad()\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        loss = cel(outputs,targets)\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        #if (i + 1) % 2000 == 0:\n",
        "            #print(running_loss/2000)\n",
        "        running_loss = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDdTBzXDVXub"
      },
      "source": [
        "*Indeed the training on GPU took around 60% of the time that it took for CPU*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTOahDvK1Sqm"
      },
      "source": [
        "## Part D: Training Your Network\n",
        "\n",
        "Now you're going to fully train your network on CIFAR-10. Using SGD with a learning rate of 0.01 and a momentum parameter of 0.9 train your network for 30 epochs using a GPU. \n",
        "\n",
        "You should log the training accuracy and average loss per training input at every epoch, and test accuracy and average loss per test input every other epoch. Make sure that you are **not** training the network when evaluating the performance on testing data. \n",
        "\n",
        "If you have constructed your network and your training loop correctly training your network should take about 5-10 minutes, including the evaluation on the test data. If it is taking significantly longer than that make sure that you are utilizing the GPU correctly, and that you are not doing any redundant computations in your code. \n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY45q2Cuwn4V"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKoK0orhV0Jp"
      },
      "source": [
        "%%time\n",
        "coptim = torch.optim.SGD(net.parameters(), lr= 0.01, momentum = 0.9)\n",
        "cel = nn.CrossEntropyLoss()\n",
        "cepochs = 30\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "total_train = len(trainset)\n",
        "total_test= len(testset)\n",
        "\n",
        "for e in range(cepochs):\n",
        "   \n",
        "   runningloss = 0.0\n",
        "   loss = 0.0\n",
        "   correct = 0.0\n",
        "   for inputss, labels in trainloader:\n",
        "\n",
        "     inputss = inputss.to(device)\n",
        "     labels = labels.to(device)\n",
        "\n",
        "     coptim.zero_grad()\n",
        "     outputs = net(inputss)\n",
        "     loss = cel(outputs,labels)\n",
        "     runningloss += loss.item()\n",
        "\n",
        "     correct += torch.sum((torch.argmax(outputs, -1) == labels)).item()\n",
        "     loss.backward()\n",
        "     coptim.step()\n",
        "   \n",
        "   acc =  100 * correct/total_train\n",
        "   trueloss = runningloss/len(trainset)\n",
        "   train_loss.append(trueloss)\n",
        "   train_acc.append(acc)\n",
        "   if (e + 1) % 1 == 0:\n",
        "     print(f'[Epoch {e + 1}] Training Loss: {trueloss} Accuracy = {acc} ')\n",
        "\n",
        "   runningloss = 0.0\n",
        "   loss = 0.0\n",
        "   correct = 0.0\n",
        "   for inputss2, labels2 in testloader:\n",
        "\n",
        "     inputss2 = inputss2.to(device)\n",
        "     labels2 = labels2.to(device)\n",
        "\n",
        "     outputs2 = net(inputss2)\n",
        "     loss = cel(outputs2,labels2)\n",
        "     runningloss += loss.item()\n",
        "\n",
        "     correct += torch.sum((torch.argmax(outputs2, -1) == labels2)).item()\n",
        "\n",
        "   acc = 100 * correct/total_test\n",
        "   test_acc.append(acc)\n",
        "   trueloss = runningloss/total_test\n",
        "   test_loss.append(trueloss)\n",
        "   if (e + 1) % 2 == 0:\n",
        "     print(f'Test Loss: {trueloss}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTBmtwAkSdCa"
      },
      "source": [
        "## Part E: Examining Your Results\n",
        "\n",
        "Take the stored training and testing evaluations from the previous question and make two plots:\n",
        "  1. A plot of loss vs. time including curves for both training and testing,\n",
        "  2. A plot of accuracy vs. time including curves for both training and testing.\n",
        "\n",
        "You should see the gap between the performance of your model start to increase as training progresses. This gap is referred to as the _generalization gap_. Why do you think the generalization gap grows later in training, and why might this be an issue for neural networks specifically.\n",
        "\n",
        "\n",
        "Please use the following function to compute the accuracy of your trained network - you should **not** need to modify this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbMZij2n-gvI"
      },
      "source": [
        "def cifar_accuracy(net, loader):\n",
        "    acc = 0\n",
        "    total_num = 0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        total_num += inputs.shape[0]\n",
        "        with torch.no_grad():\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            acc += (predicted == labels).sum().item()\n",
        "\n",
        "    return acc/total_num\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRirFo4zTere"
      },
      "source": [
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Xe8_9WSllI"
      },
      "source": [
        "print(cifar_accuracy(net,trainloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b-_cf5ny7nF"
      },
      "source": [
        "print(cifar_accuracy(net,testloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WYcLev-tmIM"
      },
      "source": [
        "print(len(trainset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JYPfXMiX_5f"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "plt.plot(train_loss, alpha=0.75, label=\"train loss\")\n",
        "plt.plot(test_loss, alpha=0.75, label=\"test loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Time\")\n",
        "sns.despine()\n",
        "plt.legend()\n",
        "plt.xlim(0, 30)\n",
        "plt.ylim(0, 0.02)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTSEZeatqlCt"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "plt.plot(train_acc, alpha=0.75, label=\"training accuracy\")\n",
        "plt.plot(test_acc, alpha=0.75, label=\"testing accuracy\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Time\")\n",
        "sns.despine()\n",
        "plt.legend()\n",
        "plt.xlim(0, 30)\n",
        "plt.ylim(0, 100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbHzDO-M_5wM"
      },
      "source": [
        "## Part F: Closing the generalization gap\n",
        "\n",
        "Re-initialize and re-train your CNN in a way that will reduce the overall generalization gap. There are a lot of ways you can do this, including writing your own custom loss function to use, or simply making the right modifications to the optimizer you are using. \n",
        "\n",
        "Again make the following two plots:\n",
        "  1. A plot of loss vs. time including curves for both training and testing,\n",
        "  2. A plot of accuracy vs. time including curves for both training and testing.\n",
        "\n",
        "What do you observe?\n",
        "\n",
        "A note: you may not see dramatic improvement here, because we are using a fairly small model, however this type of thinking and the problem of closing the generalization gap is an important consideration in how we train very large state of the art models.\n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBYJdublzwGF"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Iffcgnzm_h"
      },
      "source": [
        "net2 = ConvNet()\n",
        "net2.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4JsY0sw2gZk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKySvE8Tz_mG"
      },
      "source": [
        "%%time\n",
        "coptim = torch.optim.SGD(net2.parameters(), lr= 0.01, momentum = 0.5)\n",
        "cel = nn.CrossEntropyLoss()\n",
        "cepochs = 40\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "total_train = len(trainset)\n",
        "total_test= len(testset)\n",
        "\n",
        "for e in range(cepochs):\n",
        "   \n",
        "   runningloss = 0.0\n",
        "   loss = 0.0\n",
        "   correct = 0.0\n",
        "   for inputss, labels in trainloader:\n",
        "\n",
        "     inputss = inputss.to(device)\n",
        "     labels = labels.to(device)\n",
        "\n",
        "     coptim.zero_grad()\n",
        "     outputs = net2(inputss)\n",
        "     loss = cel(outputs,labels)\n",
        "     runningloss += loss.item()\n",
        "\n",
        "     correct += torch.sum((torch.argmax(outputs, -1) == labels)).item()\n",
        "     loss.backward()\n",
        "     coptim.step()\n",
        "   \n",
        "   acc =  100 * correct/total_train\n",
        "   trueloss = runningloss/len(trainset)\n",
        "   train_loss.append(trueloss)\n",
        "   train_acc.append(acc)\n",
        "   if (e + 1) % 1 == 0:\n",
        "     print(f'[Epoch {e + 1}] Training Loss: {trueloss} Accuracy = {acc} ')\n",
        "\n",
        "   runningloss = 0.0\n",
        "   loss = 0.0\n",
        "   correct = 0.0\n",
        "   for inputss2, labels2 in testloader:\n",
        "\n",
        "     inputss2 = inputss2.to(device)\n",
        "     labels2 = labels2.to(device)\n",
        "\n",
        "     outputs2 = net2(inputss2)\n",
        "     loss = cel(outputs2,labels2)\n",
        "     runningloss += loss.item()\n",
        "\n",
        "     correct += torch.sum((torch.argmax(outputs2, -1) == labels2)).item()\n",
        "\n",
        "   acc = 100 * correct/total_test\n",
        "   test_acc.append(acc)\n",
        "   trueloss = runningloss/total_test\n",
        "   test_loss.append(trueloss)\n",
        "   if (e + 1) % 2 == 0:\n",
        "     print(f'Test Loss: {trueloss}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3sRmhS_0dwa"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "plt.plot(train_loss, alpha=0.75, label=\"train loss\")\n",
        "plt.plot(test_loss, alpha=0.75, label=\"test loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Time\")\n",
        "sns.despine()\n",
        "plt.legend()\n",
        "plt.xlim(0, 40)\n",
        "plt.ylim(0, 0.02)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdNwW0wB0g3-"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "plt.plot(train_acc, alpha=0.75, label=\"training accuracy\")\n",
        "plt.plot(test_acc, alpha=0.75, label=\"testing accuracy\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Time\")\n",
        "sns.despine()\n",
        "plt.legend()\n",
        "plt.xlim(0, 40)\n",
        "plt.ylim(0, 100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2-Zt31974dZ"
      },
      "source": [
        "**Comments:**\n",
        "Although there was a decrease in accuracy, decreasing the momentum of SGD reduced the generalization gap. This makes sense because a higher momentum can cause overfitting in the long-run, and the same can be said about the learning rate. To compensate for the loss in accuracy, I increased the number of epochs from 30 to 40 and got to 66% accuracy versus the 70% accuracy from Part D. I could probably achieve an even higher accuracy by adding more epochs, but clearly that is very expensive and would take forever to run. From this, we observe trade-offs between accuracy, space/time efficiency, and the generalization gap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omhuO6gviAvV"
      },
      "source": [
        "## Part G: Final Step!\n",
        "\n",
        "Now that we've seen how well even simple CNNs can perform at image classification, let's go back and use our trained models to understand what convolutional layers are doing to help classify images. \n",
        "\n",
        "In this step you'll need to take a single image and pass it through just the first convolutional layer in your _trained_ network. \n",
        "The input to your first convolutional layer should have dimensions $1 \\times 3 \\times 32 \\times 32$ and the output should have dimensions $1 \\times 8 \\times 30 \\times 30$. \n",
        "\n",
        "Now, take your output and make a figure containing $9$ subplots in a $3 \\times 3$ grid. The first entry in the subplot should be the image you passed through your convolutional layer, and the $8$ remaining subplots should contain each of the channels output by your convolutional layer as $30\\times 30$ images.\n",
        "\n",
        "You should see that each of the channels output by the convolutional filter highlights a different component of the input image - this behavior is central to the success of CNN! They are able to decompose the image into distinct features and use those features to classify images. Once you have your code working try generating your plot with different input images and see what patterns you can pick up on. Note that not all examples will look great, since the images are very low resolution. And congratulations on building and training your first neural networks!\n",
        "\n",
        "**Hints:** If you have an $n \\times m$ array you can plot it using the `plt.imshow` command, and if you have an $n \\times m \\times 3$ array, using this same command will generate a color plot where the last dimension is interepreted as the RGB channels. You will need to normalize your tensors so that their entries are in $[0, 1]$ for the plots to be rendered correctly.\n",
        "\n",
        "Thus, to plot your input image which should be a torch tensor with dimensions $1 \\times 3 \\times 32 \\times 32$, after you normalize it you can call:\n",
        "```\n",
        "plt.imshow(normalized_input[0].transpose(2, 1).transpose(0, 2))\n",
        "```\n",
        "\n",
        "**Answer Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlh83i8S0a0s"
      },
      "source": [
        "\n",
        "for inputs, targets in trainloader:\n",
        "  break\n",
        "\n",
        "img = inputs[0].view(1,3,32,32).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unQzGyKmS82q"
      },
      "source": [
        "plt.imshow(inputs[0].transpose(2, 1).transpose(0, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kkjRa_2R5dG"
      },
      "source": [
        "weight = net.conv8.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB0H-NatKn0q"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,weightt):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,8,3, bias=False)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv1.weight = weightt\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cois1DhXNun7"
      },
      "source": [
        "cnn = CNN(weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mN5bouUUZpt"
      },
      "source": [
        "weight.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnJp7ygiUa2j"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPm4d4tbTd9g"
      },
      "source": [
        "img2 = cnn(img).cpu().detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QCp6YcET9sf"
      },
      "source": [
        "img2 = img2 - torch.min(img2)\n",
        "img2 /= torch.max(img2)\n",
        "img2 = img2.view(8,30,30).numpy()\n",
        "img2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpNSGnaDWjif"
      },
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(7, 7))\n",
        "\n",
        "axes[0,0].imshow(inputs[0].transpose(2, 1).transpose(0, 2))\n",
        "axes[0,1].imshow(img2[0])\n",
        "axes[0,2].imshow(img2[1])\n",
        "axes[1,0].imshow(img2[2])\n",
        "axes[1,1].imshow(img2[3])\n",
        "axes[1,2].imshow(img2[4])\n",
        "axes[2,0].imshow(img2[5])\n",
        "axes[2,1].imshow(img2[6])\n",
        "axes[2,2].imshow(img2[7])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpxl1p_TXKm5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}